model_name: "google/gemma-2-9b"
use_flash_attention: True
device: "cuda"
torch_dtype: "float16" 
layers: 32
module_names:  
  - attention
  - mlp
  - hidden