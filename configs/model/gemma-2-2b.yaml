model_name: "google/gemma-2-2b"
use_flash_attention: True
device: "cuda"
torch_dtype: "float16"
layers: 24
module_names: 
  - attention
  - mlp
  - hidden