model_name: "google/gemma-2-2b-it"
use_flash_attention: True
device: "cuda"
torch_dtype: "float16"
layers: 24
module_names:
  - attention
  - mlp
  - hidden