model_name: "meta-llama/Llama-3.1-8B"
use_flash_attention: True
device: "cuda"
torch_dtype: "float16" 
layers: 32
module_names:  
  - attention
  - mlp
  - hidden